{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjCc3igB1dj2",
    "outputId": "f504e9d9-68ca-4868-83ca-500ccd01f495"
   },
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade pip\n",
    "# !pip3 install --upgrade wandb nsepy keras pandas sklearn imageio pprint mplcursors tqdm livelossplot keras-tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import ssl\n",
    "from requests.exceptions import ConnectionError\n",
    "from http.client import RemoteDisconnected\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from ssl import SSLError\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM,Dropout,Input,Flatten,Activation,LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard \n",
    "import tensorflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "import nsepy\n",
    "import pprint\n",
    "import os\n",
    "from datetime import date,timedelta\n",
    "import datetime\n",
    "import mplcursors\n",
    "plt.style.use('fivethirtyeight')\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import livelossplot\n",
    "import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "homediraddr = '/home/hemangjoshi37a_gmail_com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"test\",magic=True,sync_tensorboard=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datahj={}\n",
    "# with open('/home/hemangjoshi37a_gmail_com/data.p', 'rb') as fp:\n",
    "#     datahj = pickle.load(fp)\n",
    "\n",
    "# #Remove empty things\n",
    "# removables=[]\n",
    "# for onename in datahj.keys():\n",
    "#     nparrayy=np.array(datahj[onename])\n",
    "#     if(nparrayy.size==0):\n",
    "#         removables.append(onename)\n",
    "        \n",
    "# for oneremovename in tqdm(removables):\n",
    "#     del datahj[oneremovename]\n",
    "    \n",
    "with open('x_train.p', 'rb') as fp:\n",
    "    x_train = pickle.load(fp)\n",
    "\n",
    "with open('y_train.p', 'rb') as fp:\n",
    "    y_train = pickle.load(fp)\n",
    "    \n",
    "with open('scale_dict.p', 'rb') as fp:\n",
    "    scale_dict = pickle.load(fp)\n",
    "    \n",
    "with open(homediraddr+'/pfiles'+'/TATASTEEL_DF.p', 'rb') as fp:\n",
    "    TATASTEEL_DF = pickle.load(fp)\n",
    "    \n",
    "# model=load_model('current34.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape :  (14711, 60, 1)\n",
      "x_train shape :  (475613, 60, 1)\n"
     ]
    }
   ],
   "source": [
    "# stock_name='TATASTEEL'\n",
    "# df = nsepy.get_history(symbol=stock_name, start=date(2012,1,1), end=date.today())\n",
    "\n",
    "# df=datahj['TATASTEEL']\n",
    "df=TATASTEEL_DF\n",
    "\n",
    "data = df.filter(['close'])  #old_line\n",
    "\n",
    "scaler=scale_dict['TATASTEEL']\n",
    "\n",
    "dataset = data.values\n",
    "training_data_len = math.ceil( len(dataset) * .97 )\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# scaler=scaler.fit(dataset)\n",
    "scaled_data = scaler.transform(dataset)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "train_data = scaled_data[0:training_data_len , :]\n",
    "for i in range(60, len(train_data)):\n",
    "  x_train.append(train_data[i-60:i, :])\n",
    "  y_train.append(train_data[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "test_data = scaled_data[training_data_len - 60: , :]\n",
    "x_test = []\n",
    "y_test = dataset[training_data_len:, :]\n",
    "for i in range(60, len(test_data)):\n",
    "  x_test.append(test_data[i-60:i, :])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "print('x_test shape : ',x_test.shape)\n",
    "print('x_train shape : ',x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df=pd.DataFrame()\n",
    "\n",
    "for onekeynameip in tqdm(datahj.keys()):\n",
    "    this_data_close_mean = datahj[onekeynameip]['Close'].mean()\n",
    "    this_data_range=0\n",
    "    if(this_data_close_mean<10 and this_data_close_mean>0):\n",
    "        this_data_range=1\n",
    "    elif(this_data_close_mean<100 and this_data_close_mean>10):\n",
    "        this_data_range=2\n",
    "    elif(this_data_close_mean<1000 and this_data_close_mean>100):\n",
    "        this_data_range=3\n",
    "    elif(this_data_close_mean<10000 and this_data_close_mean>1000):\n",
    "        this_data_range=4\n",
    "    else:\n",
    "        this_data_range=5\n",
    "        \n",
    "#     print('Mean : ',this_data_close_mean)\n",
    "#     print('Range : ', this_data_range)\n",
    "#     print()\n",
    "    avg_df= avg_df.append({\n",
    "        \n",
    "        'Symbol':[datahj[onekeynameip]['Symbol'][0]],\n",
    "        'close_mean':this_data_close_mean,\n",
    "        'data_range':this_data_range\n",
    "        \n",
    "    }\n",
    "        , ignore_index=True)\n",
    "    \n",
    "    \n",
    "print(avg_df)\n",
    "\n",
    "# Index(['Symbol', 'Series', 'Prev Close', 'Open', 'High', 'Low', 'Last',\n",
    "#    'Close', 'VWAP', 'Volume', 'Turnover', 'Trades', 'Deliverable Volume',\n",
    "#    '%Deliverble'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_1_df=avg_df.loc[avg_df['data_range'] == 1]\n",
    "range_2_df=avg_df.loc[avg_df['data_range'] == 2]\n",
    "range_3_df=avg_df.loc[avg_df['data_range'] == 3]\n",
    "range_4_df=avg_df.loc[avg_df['data_range'] == 4]\n",
    "\n",
    "# print(range_1_df)\n",
    "\n",
    "range_1_train_data={}\n",
    "range_2_train_data={}\n",
    "range_3_train_data={}\n",
    "range_4_train_data={}\n",
    "\n",
    "for onerangedatakey in tqdm(range_1_df['Symbol'].to_list()):\n",
    "    try:\n",
    "        range_1_train_data[onerangedatakey[0]]=datahj[onerangedatakey[0]]\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "for onerangedatakey in tqdm(range_2_df['Symbol'].to_list()):\n",
    "    try:\n",
    "        range_2_train_data[onerangedatakey[0]]=datahj[onerangedatakey[0]]\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "for onerangedatakey in tqdm(range_3_df['Symbol'].to_list()):\n",
    "    try:\n",
    "        range_3_train_data[onerangedatakey[0]]=datahj[onerangedatakey[0]]\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "for onerangedatakey in tqdm(range_4_df['Symbol'].to_list()):\n",
    "    try:\n",
    "        range_4_train_data[onerangedatakey[0]]=datahj[onerangedatakey[0]]\n",
    "    except KeyError:\n",
    "        continue\n",
    "    \n",
    "# print(range_1_train_data.keys())\n",
    "# print()\n",
    "# print(range_2_train_data.keys())\n",
    "# print()\n",
    "# print(range_3_train_data.keys())\n",
    "# print()\n",
    "# print(range_4_train_data.keys())\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WRHDMRdD3Adr",
    "outputId": "c15ac9d0-b9ae-49a3-e279-b883002e56d6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'range_3_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c0f1518db0ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdatahj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange_3_train_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclose_column_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0monestockname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatahj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclose_column_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0monestockname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatahj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0monestockname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Close'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'range_3_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "datahj=range_3_train_data\n",
    "\n",
    "close_column_data={}\n",
    "for onestockname in tqdm(datahj.keys()):\n",
    "    close_column_data[onestockname]=datahj[onestockname].filter(['Close']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1L_RNTDu3gd5",
    "outputId": "af8180fe-b4d3-44aa-c727-f7f1e207009e"
   },
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "\n",
    "scaled_dataset={}\n",
    "\n",
    "scale_dict={}\n",
    "\n",
    "print(len(datahj.keys()))\n",
    "\n",
    "for onestockname in tqdm(datahj.keys()):\n",
    "    try:\n",
    "        ipval_close_column=datahj[onestockname]['Close']\n",
    "        series = Series(ipval_close_column)\n",
    "        values = series.values\n",
    "        values = values.reshape((len(values), 1))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler=scaler.fit(values)\n",
    "        scale_dict[onestockname]=scaler\n",
    "        scaled_dataset[onestockname]  = scaler.transform( close_column_data[onestockname])\n",
    "    except ValueError:\n",
    "        print( onestockname +' Value error occured')\n",
    "        empty_stocks_list.append(onestockname)\n",
    "        \n",
    "# with open('scale_dict.p', 'wb') as fp:\n",
    "#     pickle.dump(scale_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(len(scaled_dataset.keys()))\n",
    "\n",
    "empty_stocks_list=[]     \n",
    "for deletename in empty_stocks_list:\n",
    "    try:\n",
    "        del datahj[onestockname]\n",
    "    except KeyError:\n",
    "        print('keyerror ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lengths_dict={}\n",
    "\n",
    "for onestocknameipp in tqdm(scaled_dataset.keys()):\n",
    "    train_lengths_dict[onestocknameipp]=  math.ceil(len(scaled_dataset[onestocknameipp])*0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWWqVFb_3-3n",
    "outputId": "93ecb208-bf0c-4a72-c72a-8792475a24e4"
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "train_data_dict={}\n",
    "\n",
    "for onestocknameipp in tqdm(scaled_dataset.keys()):\n",
    "    train_data_dict[onestocknameipp] = scaled_dataset[onestocknameipp][0:train_lengths_dict[onestocknameipp] , :]\n",
    "    for ii in range(60, len(train_data_dict[onestocknameipp])):\n",
    "        x_train.append(train_data_dict[onestocknameipp][ii-60:ii, :])\n",
    "        y_train.append(train_data_dict[onestocknameipp][ii, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "id": "DBf5XSfM5RZN"
   },
   "outputs": [],
   "source": [
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[1], x_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('x_train.p', 'wb') as fp:\n",
    "#     pickle.dump(x_train, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('y_train.p', 'wb') as fp:\n",
    "#     pickle.dump(y_train, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(50, return_sequences=True, input_shape= (60,5), kernel_initializer='glorot_normal'))\n",
    "# model.add(LSTM(50, return_sequences= True, kernel_initializer='glorot_normal'))\n",
    "# model.add(LSTM(50, return_sequences= True, kernel_initializer='glorot_normal'))\n",
    "# model.add(LSTM(50, return_sequences= True, kernel_initializer='glorot_normal'))\n",
    "# model.add(LSTM(50, return_sequences= True, kernel_initializer='glorot_normal'))\n",
    "# model.add(LSTM(50, return_sequences= True, kernel_initializer='glorot_normal'))\n",
    "# model.add(LSTM(50, return_sequences= True, kernel_initializer='glorot_normal'))\n",
    "# model.add(LSTM(50, return_sequences= True, kernel_initializer='glorot_normal'))\n",
    "# model.add(LSTM(50, return_sequences= True, kernel_initializer='glorot_normal'))\n",
    "# model.add(LSTM(50, return_sequences= True, kernel_initializer='glorot_normal'))\n",
    "# model.add(LSTM(50, return_sequences= False, kernel_initializer='glorot_normal'))\n",
    "# model.add(Dense(60,activation='softmax', kernel_initializer='glorot_normal'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 60, 4)]           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 60, 50)            11000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Tanh (TensorFlow [(None, 60, 50)]          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Tanh_1 (TensorFl [(None, 60, 50)]          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 60, 50)            20200     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Tanh_2 (TensorFl [(None, 60, 50)]          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 60, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Tanh_3 (TensorFl [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 60)                3060      \n",
      "=================================================================\n",
      "Total params: 195,860\n",
      "Trainable params: 195,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input_shape = x_train.shape[1:]\n",
    "input_shape=(60,4)\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "x = LSTM((50), return_sequences=True, kernel_initializer='glorot_normal')(inputs)\n",
    "y = LSTM(50, return_sequences=True, kernel_initializer='glorot_normal')(x)\n",
    "y = LSTM(50, return_sequences=True, kernel_initializer='glorot_normal')(y)\n",
    "# x = tensorflow.keras.layers.add([x, y])\n",
    "x = tensorflow.keras.activations.tanh(y)\n",
    "# x =  Dense(40, kernel_initializer='glorot_normal')(x)\n",
    "x = tensorflow.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "x = LSTM(50, return_sequences=True, kernel_initializer='glorot_normal')(x)\n",
    "y = LSTM(50, return_sequences=True, kernel_initializer='glorot_normal')(x)\n",
    "y = LSTM(50, return_sequences=True, kernel_initializer='glorot_normal')(y)\n",
    "# x = tensorflow.keras.layers.add([x, y])\n",
    "x = tensorflow.keras.activations.tanh(y)\n",
    "# x =  Dense(40, kernel_initializer='glorot_normal')(x)\n",
    "x = tensorflow.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "x = LSTM(50, return_sequences=True, kernel_initializer='glorot_normal')(x)\n",
    "y = LSTM(50, return_sequences=True, kernel_initializer='glorot_normal')(x)\n",
    "y = LSTM(50, return_sequences=True, kernel_initializer='glorot_normal')(y)\n",
    "# x = tensorflow.keras.layers.add([x, y])\n",
    "x = tensorflow.keras.activations.tanh(y)\n",
    "# x =  Dense(40, kernel_initializer='glorot_normal')(x)\n",
    "x = tensorflow.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "x =  LSTM(50, return_sequences=False, kernel_initializer='glorot_normal')(x)\n",
    "x = tensorflow.keras.activations.tanh(x)\n",
    "\n",
    "outputs = Dense(60, kernel_initializer='glorot_normal')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_yhADCRm6YEj",
    "outputId": "2b6e9066-2576-444b-c83b-d33b6d113cff"
   },
   "outputs": [],
   "source": [
    "# # model.compile(optimizer='adam', loss='mean_squared_error',metrics=[\"accuracy\"])\n",
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(), loss='mean_squared_error',metrics=[\"accuracy\"])\n",
    "# # model.compile(optimizer=keras.optimizers.adam(lr=0.0005, beta_1=0.9, beta_2=0.999,  decay=0.01, amsgrad=True), loss='mean_squared_error',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('/home/hemangjoshi37a_gmail_com/stkwise_models/TATASTEEL/TATASTEEL_182.h5')\n",
    "model.save('/home/hemangjoshi37a_gmail_com/stkwise_models60/TATASTEEL/TATASTEEL_26.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "homediraddr='/home/hemangjoshi37a_gmail_com'\n",
    "\n",
    "def print_dim_wise_plots():\n",
    "    weights_model_ip= model.get_weights()\n",
    "\n",
    "    list_of_shapes=[]\n",
    "\n",
    "    for oneweightnumberip in range(len(weights_model_ip)):\n",
    "        if(weights_model_ip[oneweightnumberip].shape not in list_of_shapes):\n",
    "            list_of_shapes.append(weights_model_ip[oneweightnumberip].shape)\n",
    "\n",
    "    shape_wise_weight_dict={}\n",
    "\n",
    "    for oneweightnumberip in range(len(weights_model_ip)):\n",
    "        for oneshape in list_of_shapes:\n",
    "            if(oneshape == weights_model_ip[oneweightnumberip].shape):\n",
    "                if oneshape not in shape_wise_weight_dict.keys():\n",
    "                    shape_wise_weight_dict[oneshape]=pd.DataFrame()\n",
    "                shape_wise_weight_dict[oneshape]=shape_wise_weight_dict[oneshape].append(pd.DataFrame(weights_model_ip[oneweightnumberip]))\n",
    "\n",
    "    types_of_plot=['line','hist','box','density','area','pie','scatter','hexbin']\n",
    "    plt.figure()\n",
    "    for onetypeofplot in types_of_plot:\n",
    "        for oneshapeofweights in shape_wise_weight_dict.keys():\n",
    "            try:\n",
    "                shape_wise_weight_dict[oneshapeofweights].plot(legend=False,figsize=(12,5), \n",
    "                                                               kind=onetypeofplot);\n",
    "                dirlist1= os.listdir(homediraddr+'/figs/png/')\n",
    "                dirpathip=''\n",
    "                if(str(onetypeofplot)+'_'+ str(oneshapeofweights) not in dirlist1):\n",
    "                    os.mkdir(homediraddr+'/figs/png/'+str(onetypeofplot)+'_'+ str(oneshapeofweights)) \n",
    "                dirpathip=homediraddr+'/figs/png/'+str(onetypeofplot)+'_'+ str(oneshapeofweights)+'/'\n",
    "                \n",
    "                filenum=0\n",
    "                filecheckpath = dirpathip+str(onetypeofplot)+'_'+ str(oneshapeofweights)+'_'+str(filenum)+'.png'\n",
    "                while os.path.isfile(filecheckpath):\n",
    "                    filenum+=1\n",
    "                    filecheckpath =  dirpathip+str(onetypeofplot)+'_'+ str(oneshapeofweights)+'_'+str(filenum)+'.png'\n",
    "                \n",
    "                plt.savefig(dirpathip+str(onetypeofplot)+'_'+ str(oneshapeofweights)+'_'+str(filenum)+\".png\",\n",
    "                            dpi=100, bbox_inches='tight')\n",
    "            except ValueError:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all the models\n",
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    counter1=0\n",
    "#   def on_train_batch_begin(self, batch, logs=None):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "#         clear_output(wait=True)\n",
    "        \n",
    "        global counter1\n",
    "        counter1+=1\n",
    "        if(counter1%500==0):\n",
    "            plot_predict()\n",
    "#         livelossplot.PlotLossesKeras().on_train_batch_end(batch)\n",
    "        \n",
    "        \n",
    "        if(counter1%2500==0):\n",
    "            print_dim_wise_plots()\n",
    "            \n",
    "#             model_weights_df=pd.DataFrame()\n",
    "#             weights_model= model.get_weights()\n",
    "#             for onelayerweight in weights_model:\n",
    "#                 try:\n",
    "#                     model_weights_df=model_weights_df.append(pd.DataFrame(onelayerweight))\n",
    "#                 except TypeError:\n",
    "#                     continue\n",
    "\n",
    "    #         filenum=0\n",
    "    #         filecheckpath = '/floyd/home/model_csv/live2/csv/'+str(filenum)+'.csv'\n",
    "    #         while os.path.isfile(filecheckpath):\n",
    "    #             filenum+=1\n",
    "    #             filecheckpath = '/floyd/home/model_csv/live2/csv/'+str(filenum)+'.csv'\n",
    "    #         model_weights_df.to_csv('/floyd/home/model_csv2/live/csv/' + str(filenum) +  \".csv\")\n",
    "\n",
    "\n",
    "        if(counter1%2500==0):\n",
    "            filenum=0\n",
    "            filecheckpath =homediraddr+'/figs/models/'+str(filenum)+'.h5'\n",
    "            while os.path.isfile(filecheckpath):\n",
    "                filenum+=1\n",
    "                filecheckpath = homediraddr+'/figs/models/'+str(filenum)+'.h5'\n",
    "            model.save(homediraddr+'/figs/models/'+str(filenum)+'.h5')\n",
    "\n",
    "\n",
    "#   def on_test_batch_begin(self, batch, logs=None):\n",
    "#   def on_test_batch_end(self, batch, logs=None):\n",
    "\n",
    "counter1=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_predict():\n",
    "#     global model\n",
    "    \n",
    "#     onemodelnameip='TATASTEEL'\n",
    "#     apple_quote=datahj['TATASTEEL']\n",
    "#     new_df = apple_quote.filter(['Close'])\n",
    "    \n",
    "#     scaler=scale_dict['TATASTEEL']\n",
    "    \n",
    "#     last_60_days = new_df[-120:-60].values\n",
    "#     predicted_plot=last_60_days\n",
    "#     actual_plot=new_df[-120:].values\n",
    "#     try:\n",
    "#         last_60_days_scaled = scaler.transform(last_60_days)\n",
    "#     except ValueError:\n",
    "#         _=0\n",
    "#     for jk in range(60):\n",
    "#         X_test = []\n",
    "#         X_test.append(last_60_days_scaled)\n",
    "#         X_test = np.array(X_test)\n",
    "#         X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "#         pred_price = model.predict(X_test)\n",
    "#         pred_price_descaled = scaler.inverse_transform(pred_price)\n",
    "#         predicted_plot=np.append(predicted_plot,pred_price_descaled)\n",
    "#         array_abcd=np.array(pred_price)\n",
    "#         asdasdasd=np.append(X_test,array_abcd)\n",
    "#         asdasdasd=np.delete(asdasdasd, 0,None)\n",
    "#         last_60_days_scaled = np.reshape(asdasdasd, (X_test.shape[0], X_test.shape[1], 1 ))[0]\n",
    "#     plot_df=pd.DataFrame(predicted_plot.T)\n",
    "#     plot_df2=pd.DataFrame(actual_plot)\n",
    "#     result = pd.concat([plot_df, plot_df2], axis=1, sort=False)\n",
    "# #     plt.figure(figsize=(16,8))\n",
    "# #     plt.autoscale(enable=True, axis='both', tight=None)\n",
    "\n",
    "#     result.plot(figsize=(11,5))\n",
    "#     # plt.text(0, 0, onestkstk+' : '+onemodelnameip)\n",
    "# #     plt.text(0, 0, str(filenum)+' : '+onemodelnameip)\n",
    "\n",
    "#     filenum=0\n",
    "#     filecheckpath =homediraddr+'/figs/plots/'+'TATASTEEL'+'_'+str(filenum)+'.png'\n",
    "#     while os.path.isfile(filecheckpath):\n",
    "#         filenum+=1\n",
    "#         filecheckpath =homediraddr+'/figs/plots/'+'TATASTEEL'+'_'+str(filenum)+'.png'\n",
    "#     plt.savefig(homediraddr+'/figs/plots/'+'TATASTEEL'+'_'+str(filenum)+'.png', dpi=100, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predict():\n",
    "    global model\n",
    "    \n",
    "    onemodelnameip='TATASTEEL'\n",
    "    apple_quote=df\n",
    "    new_df = apple_quote.filter(['Close'])\n",
    "    \n",
    "    scaler=scale_dict['TATASTEEL']\n",
    "    \n",
    "    last_60_days = new_df[-180:-120].values\n",
    "    predicted_plot=last_60_days\n",
    "    actual_plot=new_df[-180:].values\n",
    "    try:\n",
    "        last_60_days_scaled = scaler.transform(last_60_days)\n",
    "    except ValueError:\n",
    "        _=0\n",
    "    for jk in range(120):\n",
    "        X_test = []\n",
    "        X_test.append(last_60_days_scaled)\n",
    "        X_test = np.array(X_test)\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "        pred_price = model.predict(X_test)\n",
    "        pred_price_descaled = scaler.inverse_transform(pred_price)\n",
    "        predicted_plot=np.append(predicted_plot,pred_price_descaled)\n",
    "        array_abcd=np.array(pred_price)\n",
    "        asdasdasd=np.append(X_test,array_abcd)\n",
    "        asdasdasd=np.delete(asdasdasd, 0,None)\n",
    "        last_60_days_scaled = np.reshape(asdasdasd, (X_test.shape[0], X_test.shape[1], 1 ))[0]\n",
    "    plot_df=pd.DataFrame(predicted_plot.T)\n",
    "    plot_df2=pd.DataFrame(actual_plot)\n",
    "    result = pd.concat([plot_df, plot_df2], axis=1, sort=False)\n",
    "#     plt.figure(figsize=(16,8))\n",
    "#     plt.autoscale(enable=True, axis='both', tight=None)\n",
    "\n",
    "    result.plot(figsize=(11,5));\n",
    "    # plt.text(0, 0, onestkstk+' : '+onemodelnameip)\n",
    "#     plt.text(0, 0, str(filenum)+' : '+onemodelnameip)\n",
    "\n",
    "    filenum=0\n",
    "    filecheckpath =homediraddr+'/figs/plots/'+'TATASTEEL'+'_'+str(filenum)+'.png'\n",
    "    while os.path.isfile(filecheckpath):\n",
    "        filenum+=1\n",
    "        filecheckpath =homediraddr+'/figs/plots/'+'TATASTEEL'+'_'+str(filenum)+'.png'\n",
    "    plt.savefig(homediraddr+'/figs/plots/'+'TATASTEEL'+'_'+str(filenum)+'.png', dpi=100, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_2Qi1brn6i5L",
    "outputId": "d5662eca-1566-4fe0-e9f5-dbaa70caff67",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'last_60_days_scaled' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ef3860abfebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m history=model.fit(x_train,\n\u001b[1;32m      3\u001b[0m                   \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-673b144f161e>\u001b[0m in \u001b[0;36mplot_predict\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mjk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_60_days_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'last_60_days_scaled' referenced before assignment"
     ]
    }
   ],
   "source": [
    "plot_predict()\n",
    "history=model.fit(x_train,\n",
    "                  y_train,\n",
    "                  validation_split=0.03,\n",
    "                  verbose=1,\n",
    "                  batch_size=1, \n",
    "                  epochs=1,\n",
    "                  callbacks=[\n",
    "                             livelossplot.PlotLossesKeras(),\n",
    "#                               wandb.keras.WandbCallback(),\n",
    "                             MyCustomCallback()\n",
    "                            ])\n",
    "plot_predict()\n",
    "model.save('current35_TATASTEEL_alldata.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all model names to iterate and load\n",
    "\n",
    "from os import listdir\n",
    "path = '/floyd/home'\n",
    "def list_files1(directory, extension):\n",
    "    return (f for f in listdir(directory) if f.endswith('.' + extension))\n",
    "directory = path\n",
    "files = list_files1(directory, \"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all the models\n",
    "\n",
    "types_of_plot=['line','hist','box','kde','density','area','pie','scatter','hexbin']\n",
    "\n",
    "for onemodelname in tqdm(files):\n",
    "    model=load_model(onemodelname)\n",
    "    \n",
    "    model_weights_df=pd.DataFrame()\n",
    "    weights_model= model.get_weights()\n",
    "    for onelayerweight in weights_model:\n",
    "        try:\n",
    "            model_weights_df=model_weights_df.append(pd.DataFrame(onelayerweight))\n",
    "        except TypeError:\n",
    "            print('Type Error in weights : '+onelayerweight)\n",
    "            continue\n",
    "\n",
    "    model_weights_df.to_csv('/floyd/home/model_csv/csv/'+ onemodelname +  \".csv\")\n",
    "    for onetypeofplot in types_of_plot:\n",
    "        try:\n",
    "            model_weights_df.plot(legend=False,figsize=(10,5), kind=onetypeofplot)\n",
    "            plt.savefig('/floyd/home/model_csv/png/'+onetypeofplot+'_'+ onemodelname+\".png\",\n",
    "                        dpi=150, bbox_inches='tight')\n",
    "        except ValueError:\n",
    "            print('Value Error on plot : '+onetypeofplot+ ' : ' +onemodelname)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('resnet_3_45.h5')\n",
    "model.save(os.path.join(wandb.run.dir, \"model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenum=0\n",
    "filecheckpath = '/home/hemang/saved_models/'+'saved_model'+str(filenum)+'.h5'\n",
    "\n",
    "while os.path.isfile(filecheckpath):\n",
    "    filenum+=1\n",
    "    filecheckpath = '/home/hemang/saved_models/'+'saved_model'+str(filenum)+'.h5'\n",
    "\n",
    "model.save(filecheckpath)\n",
    "print(filecheckpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET ALL MODELS\n",
    "\n",
    "import os\n",
    "dirlist= os.listdir('/home/hemang/wandb/')\n",
    "# print(dirlist)\n",
    "existing_list=[]\n",
    "for x in os.listdir('/home/hemang/wandb/'):\n",
    "    if(x!='settings'):\n",
    "        filepathip='/home/hemang/wandb/'+x+'/model.h5'\n",
    "        if os.path.isfile(filepathip):\n",
    "            existing_list.append(filepathip)\n",
    "\n",
    "print(existing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyzer Function\n",
    "\n",
    "def analyze_model_performance(valid,one_model_path,stock_name):\n",
    "    accuracy=0\n",
    "    predicted_ok=0\n",
    "    predicted_notok=0\n",
    "    total_inc_pred=0\n",
    "    total_inc_close=0\n",
    "    total_dnc_pred=0\n",
    "    total_dnc_close=0\n",
    "    total_diff_INC=0\n",
    "    total_diff_DEC=0\n",
    "    percent_error_INC=0\n",
    "    percent_error_DEC=0\n",
    "\n",
    "    for iii in range(len(valid)-1):\n",
    "        temp_predict_inc=0\n",
    "        temp_close_inc=0\n",
    "        if(valid['Predictions'][iii]>valid['Predictions'][iii+1]):\n",
    "            temp_predict_inc=0\n",
    "            total_dnc_pred+=valid['Predictions'][iii+1]-valid['Predictions'][iii]\n",
    "        else:\n",
    "            temp_predict_inc=1\n",
    "            total_inc_pred+=valid['Predictions'][iii+1]-valid['Predictions'][iii]\n",
    "\n",
    "        if(valid['Close'][iii]>valid['Close'][iii+1]):\n",
    "            temp_close_inc=0\n",
    "            total_dnc_close+=valid['Close'][iii+1]-valid['Close'][iii]\n",
    "        else:\n",
    "            temp_close_inc=1\n",
    "            total_inc_close+=valid['Close'][iii+1]-valid['Close'][iii]\n",
    "\n",
    "        if(temp_close_inc==temp_predict_inc):\n",
    "            predicted_ok+=1\n",
    "        else:\n",
    "            predicted_notok+=1\n",
    "\n",
    "        \n",
    "    total_diff_INC=total_inc_pred-total_inc_close\n",
    "    total_diff_DEC=total_dnc_pred-total_dnc_close\n",
    "    \n",
    "    if(total_inc_close!=0):\n",
    "        percent_error_INC=100*(total_inc_pred-total_inc_close)/total_inc_close\n",
    "    else:\n",
    "        percent_error_INC='INF'\n",
    "    \n",
    "    if(total_dnc_close!=0):\n",
    "        percent_error_DEC=100*(total_dnc_pred-total_dnc_close)/total_dnc_close\n",
    "    else:\n",
    "        percent_error_DEC='INF'\n",
    "    \n",
    "    accuracy=100*predicted_ok/(predicted_ok+predicted_notok)\n",
    "    \n",
    "    return {\n",
    "        'accuracy':accuracy,\n",
    "        'predicted_ok' :   predicted_ok,\n",
    "        'predicted_notok':predicted_notok,\n",
    "        'total_inc_pred':total_inc_pred,\n",
    "        'total_inc_close':total_inc_close,\n",
    "        'total_dnc_pred':total_dnc_pred,\n",
    "        'total_dnc_close':total_dnc_close,\n",
    "        'total_diff_INC':total_diff_INC,\n",
    "        'total_diff_DEC':total_diff_DEC,\n",
    "        'percent_error_INC':percent_error_INC,\n",
    "        'percent_error_DEC':percent_error_DEC,\n",
    "        'valid_mean_diff':[valid.mean()['diff']],\n",
    "        'valid_max_diff':[valid.max()['diff']],\n",
    "        'valid_min_diff':[valid.min()['diff']],\n",
    "        'path':one_model_path,\n",
    "        'stock_name':stock_name,\n",
    "    }\n",
    "#Analyzer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Wise Analysis\n",
    "\n",
    "perforamnce_table = {\n",
    "        'accuracy':[],\n",
    "        'stock_name':[],\n",
    "        'path':[],\n",
    "        'valid_mean_diff':[],\n",
    "        'valid_max_diff':[],\n",
    "        'valid_min_diff':[],\n",
    "        'predicted_ok' :[],\n",
    "        'predicted_notok':[],\n",
    "        'total_inc_pred':[],\n",
    "        'total_inc_close':[],\n",
    "        'total_dnc_pred':[],\n",
    "        'total_dnc_close':[],\n",
    "        'total_diff_INC':[],\n",
    "        'total_diff_DEC':[],\n",
    "        'percent_error_INC':[],\n",
    "        'percent_error_DEC':[]\n",
    "}\n",
    "\n",
    "perforamnce_table_df=pd.DataFrame(perforamnce_table)\n",
    "\n",
    "for one_model_path in existing_list:\n",
    "    try:\n",
    "        model = load_model(one_model_path)\n",
    "\n",
    "        predictions = model.predict(x_test)\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "        train = data[:training_data_len]\n",
    "        valid = data[training_data_len:]\n",
    "        valid['Predictions'] = predictions\n",
    "        valid['diff']=valid['Close']-valid['Predictions']\n",
    "        valid=valid[['Close', 'Predictions','diff']]\n",
    "\n",
    "        analyzeddataget= pd.DataFrame(analyze_model_performance(valid,one_model_path))\n",
    "        \n",
    "        perforamnce_table_df=perforamnce_table_df.append(analyzeddataget,ignore_index=True)\n",
    "\n",
    "    except ValueError:\n",
    "        print('Value error found')\n",
    "\n",
    "print(perforamnce_table_df)\n",
    "perforamnce_table_df.to_csv (r'/home/hemang/model_analysis.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stockwise Analysis\n",
    "\n",
    "perforamnce_table2 = {\n",
    "        'accuracy':[],\n",
    "        'stock_name':[],\n",
    "        'path':[],\n",
    "        'valid_mean_diff':[],\n",
    "        'valid_max_diff':[],\n",
    "        'valid_min_diff':[],\n",
    "        'predicted_ok' :[],\n",
    "        'predicted_notok':[],\n",
    "        'total_inc_pred':[],\n",
    "        'total_inc_close':[],\n",
    "        'total_dnc_pred':[],\n",
    "        'total_dnc_close':[],\n",
    "        'total_diff_INC':[],\n",
    "        'total_diff_DEC':[],\n",
    "        'percent_error_INC':[],\n",
    "        'percent_error_DEC':[]\n",
    "}\n",
    "perforamnce_table_df2=pd.DataFrame(perforamnce_table2)\n",
    "\n",
    "for onestockippp in datahj.keys():\n",
    "    print(onestockippp)\n",
    "    try:\n",
    "        df5 = datahj[onestockippp]\n",
    "        data3 = df5.filter(['Close'])  #old_line\n",
    "\n",
    "        dataset = data3.values\n",
    "        training_data_len = math.ceil( len(dataset) * .97 )\n",
    "\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "        x_train3 = []\n",
    "        y_train3 = []\n",
    "        train_data = scaled_data[0:training_data_len , :]\n",
    "        for i in range(60, len(train_data)):\n",
    "          x_train3.append(train_data[i-60:i, :])\n",
    "          y_train3.append(train_data[i, 0])\n",
    "\n",
    "        x_train3, y_train3 = np.array(x_train3), np.array(y_train3)\n",
    "\n",
    "        test_data = scaled_data[training_data_len - 60: , :]\n",
    "        x_test3 = []\n",
    "        y_test3 = dataset[training_data_len:, :]\n",
    "        for i in range(60, len(test_data)):\n",
    "          x_test3.append(test_data[i-60:i, :])\n",
    "\n",
    "        x_test3 = np.array(x_test3)\n",
    "\n",
    "        x_test3 = np.reshape(x_test3, (x_test3.shape[0], x_test3.shape[1], 1 ))\n",
    "        x_test3.shape\n",
    "\n",
    "\n",
    "        ###########################################\n",
    "\n",
    "\n",
    "        predictions = model.predict(x_test3)\n",
    "        predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "        train = data3[:training_data_len]\n",
    "        valid = data3[training_data_len:]\n",
    "        valid['Predictions'] = predictions\n",
    "\n",
    "        valid['diff']=valid['Close']-valid['Predictions']\n",
    "        valid=valid[['Close', 'Predictions','diff']]\n",
    "        one_model_path='/home/hemang/wandb/run-20200407_125141-29n1reyw/model.h5'\n",
    "\n",
    "        analyzeddataget= pd.DataFrame(analyze_model_performance(valid,one_model_path,onestockippp))\n",
    "\n",
    "#         print(onestockippp , ' =:= ',analyzeddataget)\n",
    "\n",
    "        perforamnce_table_df2=perforamnce_table_df2.append(analyzeddataget,ignore_index=True)\n",
    "\n",
    "#         print('-------------------------------------------------------------------------------------------------')\n",
    "        \n",
    "    except ValueError:\n",
    "        print('Value Error Found in stockwise.')\n",
    "\n",
    "    \n",
    "print(perforamnce_table_df2)\n",
    "perforamnce_table_df2.to_csv (r'/home/hemang/model_analysis_sharewise.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock and Model Wise analysis \n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "perforamnce_table = {\n",
    "        'accuracy':[],\n",
    "        'stock_name':[],\n",
    "        'path':[],\n",
    "        'valid_mean_diff':[],\n",
    "        'valid_max_diff':[],\n",
    "        'valid_min_diff':[],\n",
    "        'predicted_ok' :[],\n",
    "        'predicted_notok':[],\n",
    "        'total_inc_pred':[],\n",
    "        'total_inc_close':[],\n",
    "        'total_dnc_pred':[],\n",
    "        'total_dnc_close':[],\n",
    "        'total_diff_INC':[],\n",
    "        'total_diff_DEC':[],\n",
    "        'percent_error_INC':[],\n",
    "        'percent_error_DEC':[]\n",
    "}\n",
    "\n",
    "perforamnce_table_df=pd.DataFrame(perforamnce_table)\n",
    "valuerrrorstoccks=[]\n",
    "valueerrormodel=[]\n",
    "\n",
    "for one_model_path in tqdm(existing_list):\n",
    "    model = load_model(one_model_path)\n",
    "    for onestockippp in datahj.keys():\n",
    "        try:\n",
    "            df5 = datahj[onestockippp]\n",
    "            data = df5.filter(['Close'])  #old_line\n",
    "\n",
    "            dataset = data.values\n",
    "            training_data_len = math.ceil( len(dataset) * .97 )\n",
    "\n",
    "            scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "            x_train = []\n",
    "            y_train = []\n",
    "            train_data = scaled_data[0:training_data_len , :]\n",
    "            for i in range(60, len(train_data)):\n",
    "              x_train.append(train_data[i-60:i, :])\n",
    "              y_train.append(train_data[i, 0])\n",
    "\n",
    "            x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "            test_data = scaled_data[training_data_len - 60: , :]\n",
    "            x_test = []\n",
    "            y_test = dataset[training_data_len:, :]\n",
    "            for i in range(60, len(test_data)):\n",
    "              x_test.append(test_data[i-60:i, :])\n",
    "\n",
    "            x_test = np.array(x_test)\n",
    "\n",
    "            x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "\n",
    "            predictions = model.predict(x_test)\n",
    "            predictions = scaler.inverse_transform(predictions)\n",
    "            train = data[:training_data_len]\n",
    "            valid = data[training_data_len:]\n",
    "            valid['Predictions'] = predictions\n",
    "            valid['diff']=valid['Close']-valid['Predictions']\n",
    "            valid=valid[['Close', 'Predictions','diff']]\n",
    "            analyzeddataget= pd.DataFrame(analyze_model_performance(valid,one_model_path,onestockippp))\n",
    "            perforamnce_table_df=perforamnce_table_df.append(analyzeddataget,ignore_index=True)\n",
    "\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "print(perforamnce_table_df)\n",
    "perforamnce_table_df.to_csv (r'/home/hemang/model_analysis_all.csv', index = False, header=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final performance matrix evaluation using stock&model_wise_analysis_data\n",
    "\n",
    "perforamnce_table_df = pd.read_csv(\"model_analysis_all.csv\") \n",
    "\n",
    "# only_path_and_stkname=perforamnce_table_df\n",
    "\n",
    "perforamnce_table_df=perforamnce_table_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "new_scaled_df=perforamnce_table_df\n",
    "\n",
    "new_scaled_df['valid_mean_diff']= perforamnce_table_df['valid_mean_diff']/ max([abs(perforamnce_table_df.min()['valid_mean_diff']),abs(perforamnce_table_df.max()['valid_mean_diff'])])\n",
    "new_scaled_df['valid_mean_diff']=new_scaled_df['valid_mean_diff'].abs()\n",
    "\n",
    "new_scaled_df['valid_min_diff']= perforamnce_table_df['valid_min_diff']/ max([abs(perforamnce_table_df.min()['valid_min_diff']),abs(perforamnce_table_df.max()['valid_min_diff'])])\n",
    "new_scaled_df['valid_min_diff']=new_scaled_df['valid_min_diff'].abs()\n",
    "\n",
    "new_scaled_df['valid_max_diff']= perforamnce_table_df['valid_max_diff']/ max([abs(perforamnce_table_df.min()['valid_max_diff']),abs(perforamnce_table_df.max()['valid_max_diff'])])\n",
    "new_scaled_df['valid_max_diff']=new_scaled_df['valid_max_diff'].abs()\n",
    "\n",
    "new_scaled_df['predicted_ok']= 1-( perforamnce_table_df['predicted_ok']/perforamnce_table_df.max()['predicted_ok'])\n",
    "new_scaled_df['predicted_notok']=  perforamnce_table_df['predicted_notok']/perforamnce_table_df.max()['predicted_notok']\n",
    "\n",
    "new_scaled_df['accuracy']=1-(perforamnce_table_df['accuracy']/100)\n",
    "\n",
    "new_scaled_df['total_inc_pred']=perforamnce_table_df['total_inc_pred']/perforamnce_table_df.max()['total_inc_pred']\n",
    "new_scaled_df['total_dnc_pred']=perforamnce_table_df['total_dnc_pred']/perforamnce_table_df.min()['total_dnc_pred']\n",
    "\n",
    "new_scaled_df=new_scaled_df.drop(['total_dnc_close', 'total_inc_close'], axis=1)\n",
    "\n",
    "new_scaled_df['total_diff_INC']= perforamnce_table_df['total_diff_INC']/ max([abs(perforamnce_table_df.min()['total_diff_INC']),abs(perforamnce_table_df.max()['total_diff_INC'])])\n",
    "new_scaled_df['total_diff_INC']=new_scaled_df['total_diff_INC'].abs()\n",
    "\n",
    "new_scaled_df['total_diff_DEC']= perforamnce_table_df['total_diff_DEC']/ max([abs(perforamnce_table_df.min()['total_diff_DEC']),abs(perforamnce_table_df.max()['total_diff_DEC'])])\n",
    "new_scaled_df['total_diff_DEC']=new_scaled_df['total_diff_DEC'].abs()\n",
    "\n",
    "new_scaled_df['percent_error_INC']= perforamnce_table_df['percent_error_INC']/ max([abs(perforamnce_table_df.min()['percent_error_INC']),abs(perforamnce_table_df.max()['percent_error_INC'])])\n",
    "new_scaled_df['percent_error_INC']=new_scaled_df['percent_error_INC'].abs()\n",
    "\n",
    "new_scaled_df['percent_error_DEC']= perforamnce_table_df['percent_error_DEC']/ max([abs(perforamnce_table_df.min()['percent_error_DEC']),abs(perforamnce_table_df.max()['percent_error_DEC'])])\n",
    "new_scaled_df['percent_error_DEC']=new_scaled_df['percent_error_DEC'].abs()\n",
    "\n",
    "stk_path_drop=new_scaled_df.drop(['stock_name', 'path'], axis=1)\n",
    "stk_path_drop\n",
    "stk_path_drop[\"sum\"] = stk_path_drop.sum(axis=1)\n",
    "\n",
    "stk_path_drop['path']=perforamnce_table_df['path']\n",
    "stk_path_drop['stock_name']=perforamnce_table_df['stock_name']\n",
    "\n",
    "print(stk_path_drop)\n",
    "\n",
    "stk_path_drop.to_csv (r'/home/hemang/full_final.csv', index = False, header=True)  \n",
    "# print('MIN /n',stk_path_drop.min())\n",
    "# print('MAX /n',stk_path_drop.max())\n",
    "# print('MEAN /n',stk_path_drop.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check modelwise accuracy\n",
    "\n",
    "for klkl in tqdm(perforamnce_table_df.index):\n",
    "    \n",
    "model_wise_accuracy=[]\n",
    "for onemodelnn in existing_list:\n",
    "    onemodeldata=perforamnce_table_df.loc[perforamnce_table_df['path'] == onemodelnn]\n",
    "    onemodeldata.mean(axis = 0)['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the testing data set\n",
    "#Create a new array containing scaled values from index 1543 to 2002 \n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "# test_data_dict={}\n",
    "# for onestocknameipp in tqdm(scaled_dataset.keys()):\n",
    "#     test_data_dict[onestocknameipp] = scaled_dataset[onestocknameipp][0:train_lengths_dict[onestocknameipp] , :]\n",
    "#     y_test = scaled_dataset[onestocknameipp][train_lengths_dict[onestocknameipp]:, :]\n",
    "#     for ii in range(60, train_lengths_dict[onestocknameipp]):\n",
    "#         x_test.append(test_data_dict[onestocknameipp][ii-60:ii, :])\n",
    "\n",
    "test_data = scaled_data[training_data_len - 60: , :]\n",
    "#Create the data sets x_test and y_test\n",
    "x_test = []\n",
    "y_test = dataset[training_data_len:, :]\n",
    "for i in range(60, len(test_data)):\n",
    "  x_test.append(test_data[i-60:i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "x_test.shape\n",
    "\n",
    "predictions = model.predictions(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
    "rmse\n",
    "\n",
    "# train_ipp=close_column_data['MTNL'][:train_lengths_dict['MTNL']]\n",
    "# valid_ipp=close_column_data['MTNL'][train_lengths_dict['MTNL']:]\n",
    "# valid_ipp['Predictions'] = predictions\n",
    "\n",
    "#Plot the data\n",
    "train = data[:training_data_len]\n",
    "valid = data[training_data_len:]\n",
    "valid['Predictions'] = predictions\n",
    "#Visualize the data\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.title('Model')\n",
    "plt.xlabel('Date', fontsize=18)\n",
    "plt.ylabel('Close Price USD ($)', fontsize=18)\n",
    "plt.plot(train['Close'])\n",
    "plt.plot(valid[['Close', 'Predictions']])\n",
    "plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#Show the valid and predicted prices\n",
    "valid['diff']=valid['Close']-valid['Predictions']\n",
    "valid=valid[['Close', 'Predictions','diff']]\n",
    "valid.plot(figsize=(16,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "cm = metrics.confusion_matrix(test_batch.classes, y_pred)\n",
    "# or\n",
    "#cm = np.array([[1401,    0],[1112, 0]])\n",
    "\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.xlabel(\"Predicted labels\")\n",
    "plt.ylabel(\"True labels\")\n",
    "plt.xticks([], [])\n",
    "plt.yticks([], [])\n",
    "plt.title('Confusion matrix ')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rXyu5S_n-7ds",
    "outputId": "74f27fbb-94eb-41ef-a122-ed7e55034c5e"
   },
   "outputs": [],
   "source": [
    "#Get the quote\n",
    "stock_name='NIFTY50'\n",
    "apple_quote =nsepy.get_history(symbol=stock_name, start=date(2012,1,1), end=date.today())\n",
    "#Create a new dataframe\n",
    "model=load_model('/home/hemang/wandb/run-20200408_112313-r5pghjhu/model.h5')\n",
    "new_df = apple_quote.filter(['Close'])\n",
    "#Get teh last 60 day closing price values and convert the dataframe to an array\n",
    "last_60_days = new_df[-60:].values\n",
    "#Scale the data to be values between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "last_60_days_scaled = scaler.fit_transform(last_60_days)\n",
    "#Create an empty list\n",
    "X_test = []\n",
    "#Append teh past 60 days\n",
    "X_test.append(last_60_days_scaled)\n",
    "#Convert the X_test data set to a numpy array\n",
    "X_test = np.array(X_test)\n",
    "#Reshape the data\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "#Get the predicted scaled price\n",
    "pred_price = model.predict(X_test)\n",
    "#undo the scaling \n",
    "pred_price = scaler.inverse_transform(pred_price)\n",
    "print(pred_price[0],type(pred_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LErjDVJvAbQa",
    "outputId": "f62add48-67c8-42d9-9c1e-ba09acc24296"
   },
   "outputs": [],
   "source": [
    "#Get the quote\n",
    "apple_quote2 =nsepy.get_history(symbol=stock_name, start=date(2012,1,1), end=date.today())\n",
    "print(apple_quote2['Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# predict_stock_list=['GAYAPROJ','JPPOWER','SIMPLEXINF','GREENPOWER','SICAL','HDIL','AKSHOPTFBR','COX&KINGS','ALOKTEXT','FLEXITUFF']\n",
    "# predict_stock_list=newlsstttt\n",
    "# stock_get_dict_data={}\n",
    "# ipstockdata={}\n",
    "sixtydaysago = date.today()-datetime.timedelta(days = 500)\n",
    "\n",
    "# for ippstockname in tqdm(predict_stock_list):\n",
    "#     try:\n",
    "#         ipstockdata[ippstockname]=nsepy.get_history(symbol=ippstockname, start=sixtydaysago, end=date.today())\n",
    "#     except ValueError:\n",
    "#         print('Value error occured in ',ippstockname)\n",
    "#         continue\n",
    "#     except KeyError:\n",
    "#         print(\"age is unknown. c1\")\n",
    "#     except SSLError as e:\n",
    "#         print('SSL error uccured. c1')\n",
    "#     except ConnectionError as e:\n",
    "#         print('connection error. c1')\n",
    "#     except ConnectionResetError as e:\n",
    "#         print('connection reset error. c1')\n",
    "#     except AttributeError:\n",
    "#         print('attributre eror accured. c1')\n",
    "#     except RemoteDisconnected:\n",
    "#         print('Remote disconnected error c1')\n",
    "#     except ProtocolError:\n",
    "#         print('protocol error occured c1')\n",
    "        \n",
    "# print(ipstockdata)\n",
    "\n",
    "# with open('two_hun_stk_data.p', 'wb') as fp:\n",
    "#     pickle.dump(ipstockdata, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('/kaggle/input/datasetip/two_hun_stk_data.p', 'rb') as fp:\n",
    "    ipstockdata = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove empty things\n",
    "removables2=[]\n",
    "for onename in ipstockdata.keys():\n",
    "    nparrayy=np.array(ipstockdata[onename])\n",
    "    if(nparrayy.size==0):\n",
    "        removables2.append(onename)\n",
    "        \n",
    "for oneremovename in removables2:\n",
    "    del ipstockdata[oneremovename]\n",
    "    \n",
    "print(removables2)\n",
    "print(len(removables2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock prediction list wise and model wise\n",
    "\n",
    "predicteddataframe=pd.DataFrame({'stock_name':[],\n",
    "                        'model':[],\n",
    "                        'today_date': [],\n",
    "                        'predicted_price':[],\n",
    "                        'index':[]\n",
    "                      })\n",
    "\n",
    "for onepathname in tqdm(existing_list):\n",
    "\n",
    "    model = load_model(onepathname)\n",
    "    print(onepathname)\n",
    "    indexhjhj=0\n",
    "    model = load_model(onepathname)\n",
    "\n",
    "    for onnnestknaeemmeee in ipstockdata.keys():\n",
    "        try:\n",
    "            apple_quote=ipstockdata[onnnestknaeemmeee]\n",
    "            new_df = apple_quote.filter(['Close'])\n",
    "            last_60_days = new_df[-60:].values\n",
    "            scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            last_60_days_scaled = scaler.fit_transform(last_60_days)\n",
    "            X_test = []\n",
    "            X_test.append(last_60_days_scaled)\n",
    "            X_test = np.array(X_test)\n",
    "            X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "#             print(onnnestknaeemmeee, '----',X_test)\n",
    "#             print(onnnestknaeemmeee, '----',X_test.shape)\n",
    "            pred_price = model.predict(X_test)\n",
    "            pred_price = scaler.inverse_transform(pred_price)\n",
    "            data_dict={'stock_name':onnnestknaeemmeee,\n",
    "                        'model':onepathname,\n",
    "                        'today_date': date.today(),\n",
    "                        'predicted_price':pred_price[0],\n",
    "                       'index':indexhjhj\n",
    "                      }\n",
    "            indexhjhj+=1\n",
    "\n",
    "            predicteddataframe=predicteddataframe.append(pd.DataFrame(data_dict))\n",
    "    \n",
    "        except ValueError as dede:\n",
    "\n",
    "            print('Value error occured.',dede)\n",
    "            continue\n",
    "#         except KeyError:\n",
    "#             print(\"age is unknown.\")\n",
    "        except SSLError as e:\n",
    "            print('SSL error uccured.')\n",
    "        except ConnectionError as e:\n",
    "            print('connection error.')\n",
    "        except ConnectionResetError as e:\n",
    "            print('connection reset error.')\n",
    "        except AttributeError:\n",
    "            print('attributre eror accured.')\n",
    "        except RemoteDisconnected:\n",
    "            print('Remote disconnected error')\n",
    "        except ProtocolError:\n",
    "            print('protocol error occured')\n",
    "\n",
    "\n",
    "predicteddataframe\n",
    "predicteddataframe.to_excel('two_hunder_predicted.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusyerize results\n",
    "\n",
    "points=predicteddataframe.loc[predicteddataframe['stock_name'] == 'ADHUNIK']['predicted_price']\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "points_array=points.to_numpy()\n",
    "reshapen=points_array.reshape(-1,1)\n",
    "kmeans.fit(reshapen)\n",
    "print(kmeans.cluster_centers_)\n",
    "y_km = kmeans.fit_predict(reshapen)\n",
    "plt.scatter(reshapen[y_km ==0,0], reshapen[y_km == 0,0], s=100, c='red')\n",
    "plt.scatter(reshapen[y_km ==1,0], reshapen[y_km == 1,0], s=100, c='black')\n",
    "plt.scatter(reshapen[y_km ==2,0], reshapen[y_km == 2,0], s=100, c='blue')\n",
    "# plt.scatter(reshapen[y_km ==3,0], reshapen[y_km == 3,0], s=100, c='cyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock prediction list wise and model wise\n",
    "\n",
    "predicteddataframe=pd.DataFrame({'stock_name':[],\n",
    "                        'model':[],\n",
    "                        'today_date': [],\n",
    "                        'predicted_price':[],\n",
    "                        'index':[]\n",
    "                      })\n",
    "\n",
    "prediction_series={}\n",
    "\n",
    "for onepathname in tqdm(existing_list):\n",
    "\n",
    "    model = load_model(onepathname)\n",
    "    print(onepathname)\n",
    "#     model = load_model(onepathname)\n",
    "\n",
    "    for onnnestknaeemmeee in ipstockdata.keys():\n",
    "        try:\n",
    "            apple_quote=ipstockdata[onnnestknaeemmeee]\n",
    "            new_df = apple_quote.filter(['Close'])\n",
    "            last_60_days = new_df[-60:].values\n",
    "            scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            last_60_days_scaled = scaler.fit_transform(last_60_days)\n",
    "            X_test = []\n",
    "            X_test.append(last_60_days_scaled)\n",
    "            X_test = np.array(X_test)\n",
    "            X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "            pred_price = model.predict(X_test)\n",
    "            pred_price = scaler.inverse_transform(pred_price)\n",
    "            data_dict={'stock_name':onnnestknaeemmeee,\n",
    "                        'model':onepathname,\n",
    "                        'today_date': date.today(),\n",
    "                        'predicted_price':pred_price[0],\n",
    "                      }\n",
    "\n",
    "            predicteddataframe=predicteddataframe.append(pd.DataFrame(data_dict))\n",
    "    \n",
    "        except ValueError as dede:\n",
    "            print('Value error occured.',dede)\n",
    "            continue\n",
    "        except SSLError as e:\n",
    "            print('SSL error uccured.')\n",
    "        except ConnectionError as e:\n",
    "            print('connection error.')\n",
    "        except ConnectionResetError as e:\n",
    "            print('connection reset error.')\n",
    "        except AttributeError:\n",
    "            print('attributre eror accured.')\n",
    "        except RemoteDisconnected:\n",
    "            print('Remote disconnected error')\n",
    "        except ProtocolError:\n",
    "            print('protocol error occured')\n",
    "\n",
    "\n",
    "predicteddataframe\n",
    "predicteddataframe.to_excel('two_hunder_predicted.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict={}\n",
    "for onemodelnameip in  tqdm(existing_list):\n",
    "    model_dict[onemodelnameip]= load_model(onemodelnameip)\n",
    "# model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stk_name112='TATASTEEL'\n",
    "sixtydaysago2 = date.today()-datetime.timedelta(days = 500)\n",
    "apple_quote=nsepy.get_history(symbol=stk_name112, start=sixtydaysago2, end=date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "for onestkstk in tqdm(ipstockdata.keys()):\n",
    "    model_num=0\n",
    "    apple_quote=ipstockdata[onestkstk]\n",
    "    for onemodelnameip in  tqdm(model_dict.keys()):\n",
    "        model = model_dict[onemodelnameip]\n",
    "        new_df = apple_quote.filter(['Close'])\n",
    "        last_60_days = new_df[-120:-60].values\n",
    "        predicted_plot=last_60_days\n",
    "        actual_plot=new_df[-120:].values\n",
    "\n",
    "        for jk in range(60):\n",
    "            try:\n",
    "                last_60_days_scaled = scaler.fit_transform(last_60_days)\n",
    "            except ValueError:\n",
    "                continue   \n",
    "            X_test = []\n",
    "            X_test.append(last_60_days_scaled)\n",
    "            X_test = np.array(X_test)\n",
    "            X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "            try:\n",
    "                pred_price = model.predict(X_test)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            pred_price_descaled = scaler.inverse_transform(pred_price)\n",
    "            predicted_plot=np.append(predicted_plot,pred_price_descaled)\n",
    "            array_abcd=np.array(pred_price_descaled)\n",
    "            asdasdasd=np.append(X_test,array_abcd)\n",
    "            asdasdasd=np.delete(asdasdasd, 0,None)\n",
    "            last_60_days = np.reshape(asdasdasd, (X_test.shape[0], X_test.shape[1], 1 ))[0]\n",
    "\n",
    "        plot_df=pd.DataFrame(predicted_plot.T)\n",
    "        plot_df2=pd.DataFrame(actual_plot)\n",
    "        result = pd.concat([plot_df, plot_df2], axis=1, sort=False)\n",
    "        plt.figure(figsize=(16,8))\n",
    "        plt.autoscale(enable=True, axis='both', tight=None)\n",
    "        \n",
    "        result.plot(figsize=(16,8))\n",
    "        plt.text(0, 0, onestkstk+' : '+onemodelnameip)\n",
    "        plt.savefig('/home/hemang/figs/'+onestkstk+str(model_num)+'.png', dpi=200, bbox_inches='tight')\n",
    "        model_num+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Wise plotter\n",
    "\n",
    "modelnameinto='/kaggle/input/resnet-models/resnet_2_glorot_uni_20_layer_deep.h'\n",
    "\n",
    "model = load_model(modelnameinto)\n",
    "\n",
    "ipstockdata=range_2_train_data\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,5))\n",
    "for onestkstk in tqdm(ipstockdata.keys()):\n",
    "    model_num=0\n",
    "    onemodelnameip=modelnameinto\n",
    "    apple_quote=ipstockdata[onestkstk]\n",
    "#     model = load_model('model11225.h')\n",
    "    new_df = apple_quote.filter(['Close'])\n",
    "    last_60_days = new_df[-120:-60].values\n",
    "    predicted_plot=last_60_days\n",
    "    actual_plot=new_df[-120:].values\n",
    "    for jk in range(60):\n",
    "        try:\n",
    "            last_60_days_scaled = scaler.fit_transform(last_60_days)\n",
    "        except ValueError:\n",
    "            continue   \n",
    "        X_test = []\n",
    "        X_test.append(last_60_days_scaled)\n",
    "        X_test = np.array(X_test)\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "        try:\n",
    "            pred_price = model.predict(X_test)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        pred_price_descaled = scaler.inverse_transform(pred_price)\n",
    "        predicted_plot=np.append(predicted_plot,pred_price_descaled)\n",
    "        array_abcd=np.array(pred_price_descaled)\n",
    "        asdasdasd=np.append(X_test,array_abcd)\n",
    "        asdasdasd=np.delete(asdasdasd, 0,None)\n",
    "        last_60_days = np.reshape(asdasdasd, (X_test.shape[0], X_test.shape[1], 1 ))[0]\n",
    "\n",
    "    plot_df=pd.DataFrame(predicted_plot.T)\n",
    "    plot_df2=pd.DataFrame(actual_plot)\n",
    "    result = pd.concat([plot_df, plot_df2], axis=1, sort=False)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.autoscale(enable=True, axis='both', tight=None)\n",
    "\n",
    "    result.plot(figsize=(16,8))\n",
    "    plt.text(0, 0, onestkstk+' : '+onemodelnameip)\n",
    "    try:\n",
    "        plt.savefig('fig10/'+onestkstk+str(model_num)+'.png', dpi=200, bbox_inches='tight')\n",
    "    except ValueError:\n",
    "        continue\n",
    "    model_num+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Wise plotter\n",
    "import gc\n",
    "modelnameinto='TATASTEEL_MODEL'\n",
    "\n",
    "ipstockdata=datahj['TATASTEEL']\n",
    "\n",
    "for model_num  in tqdm(range(94,134)):\n",
    "    # model_num=0\n",
    "    clear_output(wait=True)\n",
    "    model_path_new='/floyd/home/model_csv/live/models/'+str(model_num)+'.h5'\n",
    "    model=load_model(model_path_new)\n",
    "    onemodelnameip=modelnameinto\n",
    "    apple_quote=ipstockdata\n",
    "    new_df = apple_quote.filter(['Close'])\n",
    "    last_60_days = new_df[-120:-60].values\n",
    "    predicted_plot=last_60_days\n",
    "    actual_plot=new_df[-120:].values\n",
    "    for jk in range(60):\n",
    "        try:\n",
    "            last_60_days_scaled = scaler.transform(last_60_days)\n",
    "        except ValueError:\n",
    "            continue   \n",
    "        X_test = []\n",
    "        X_test.append(last_60_days_scaled)\n",
    "        X_test = np.array(X_test)\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "        pred_price = model.predict(X_test)\n",
    "        pred_price_descaled = scaler.inverse_transform(pred_price)\n",
    "        predicted_plot=np.append(predicted_plot,pred_price_descaled)\n",
    "        array_abcd=np.array(pred_price_descaled)\n",
    "        asdasdasd=np.append(X_test,array_abcd)\n",
    "        asdasdasd=np.delete(asdasdasd, 0,None)\n",
    "        last_60_days = np.reshape(asdasdasd, (X_test.shape[0], X_test.shape[1], 1 ))[0]\n",
    "    plot_df=pd.DataFrame(predicted_plot.T)\n",
    "    plot_df2=pd.DataFrame(actual_plot)\n",
    "    result = pd.concat([plot_df, plot_df2], axis=1, sort=False)\n",
    "#     plt.figure(figsize=(16,8))\n",
    "#     plt.autoscale(enable=True, axis='both', tight=None)\n",
    "\n",
    "    result.plot(figsize=(16,8))\n",
    "    # plt.text(0, 0, onestkstk+' : '+onemodelnameip)\n",
    "    plt.text(0, 0, str(model_num)+' : '+onemodelnameip)\n",
    "    plt.savefig('/floyd/home/model_csv/live/plots/'+str(model_num)+'TATASTEEL.png', dpi=200, bbox_inches='tight')\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "def zipdir(path, ziph):\n",
    "    # ziph is zipfile handle\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            ziph.write(os.path.join(root, file))\n",
    "\n",
    "zipf = zipfile.ZipFile('fig17.zip', 'w', zipfile.ZIP_DEFLATED)\n",
    "zipdir('/floyd/home/fig4/', zipf)\n",
    "zipf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIF Convert script in terminal\n",
    "# convert -delay 20 -loop 0 *.jpg myimage.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from kiteconnect import KiteConnect\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "kite = KiteConnect(api_key=\"w19o0chuo929jxkp\")\n",
    "kite.login_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = kite.generate_session(\"jG2jTauB0gw6u5vZzF6oR1YZpP6I2c7V\", api_secret=\"gsw8ps17ex7lf3cuji4prfnwb4vlyr4y\")\n",
    "kite.set_access_token(data[\"access_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "delta = timedelta(days=100)\n",
    "\n",
    "todaydt=datetime.date.today()\n",
    "\n",
    "hud_ago=todaydt-delta\n",
    "\n",
    "old_lst=[]\n",
    "\n",
    "instrument_token='895745'\n",
    "interval='5minute'\n",
    "delta1 = timedelta(days=99)\n",
    "delta = timedelta(days=100)\n",
    "todaydt=datetime.date.today()\n",
    "hud_ago=todaydt-delta1\n",
    "\n",
    "to_date=datetime.date.isoformat(todaydt)\n",
    "from_date=datetime.date.isoformat(hud_ago)\n",
    "\n",
    "print(to_date)\n",
    "print(from_date)\n",
    "\n",
    "old_lst=kite.historical_data(instrument_token, from_date, to_date, interval,continuous=False)\n",
    "\n",
    "print(len(old_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i2 in tqdm(range(22)):\n",
    "\n",
    "    todaydt=todaydt-delta\n",
    "    hud_ago=hud_ago-delta\n",
    "    \n",
    "    to_date=datetime.date.isoformat(todaydt)\n",
    "    from_date=datetime.date.isoformat(hud_ago)\n",
    "    \n",
    "    new_lst = kite.historical_data(instrument_token, from_date, to_date, interval,continuous=False)\n",
    "    old_lst = new_lst + old_lst\n",
    "    \n",
    "    if(len(new_lst)==0):\n",
    "        print('empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TATASTEEL_DF=pd.DataFrame(old_lst)\n",
    "print(len(old_lst))\n",
    "TATASTEEL_DF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# inst_all=kite.instruments()\n",
    "\n",
    "# # {'exchange': 'BCD',\n",
    "# #   'exchange_token': '2111805',\n",
    "# #   'expiry': datetime.date(2020, 12, 29),\n",
    "# #   'instrument_token': 540622086,\n",
    "# #   'instrument_type': 'PE',\n",
    "# #   'last_price': 0.0,\n",
    "# #   'lot_size': 1,\n",
    "# #   'name': 'GBPINR',\n",
    "# #   'segment': 'BCD-OPT',\n",
    "# #   'strike': 93.0,\n",
    "# #   'tick_size': 0.0025,\n",
    "# #   'tradingsymbol': 'GBPINR20DEC93.0000PE'},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kite.holdings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kite.orders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TATASTEEL_DF.p', 'wb') as fp:\n",
    "    pickle.dump(TATASTEEL_DF, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TATASTEEL_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LSTM2.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 424,
   "position": {
    "height": "450px",
    "left": "372px",
    "right": "20px",
    "top": "41px",
    "width": "585px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
