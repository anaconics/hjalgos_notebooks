{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import ssl\n",
    "from requests.exceptions import ConnectionError\n",
    "from http.client import RemoteDisconnected\n",
    "from urllib3.exceptions import ProtocolError\n",
    "from ssl import SSLError\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential,Model,load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM,Dropout,Input,Flatten,Activation,LeakyReLU\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard \n",
    "import tensorflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "import nsepy\n",
    "import pprint\n",
    "import os\n",
    "from datetime import date,timedelta\n",
    "import datetime\n",
    "import mplcursors\n",
    "plt.style.use('fivethirtyeight')\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import livelossplot\n",
    "import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "homediraddr='/home/hemangjoshi37a_gmail_com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_trian_stk_class:\n",
    "    def __init__(self, stkname, insttkn,nbatch,nepochs):\n",
    "        self.name = stkname\n",
    "        self.insttkn = insttkn\n",
    "        self.nepochs=nepochs\n",
    "        self.nbatch=nbatch\n",
    "        \n",
    "    def load(self):\n",
    "        filecheckpath =homediraddr+'/pfiles/'+self.name+'_'+'DF'+'.p'\n",
    "        if(os.path.isfile(filecheckpath)):\n",
    "            with open(filecheckpath, 'rb') as fp:\n",
    "                self.df = pickle.load(fp)\n",
    "            print('Loaded : '+self.name+'_'+'DF'+'.p')\n",
    "\n",
    "        else:\n",
    "            old_lst=[]\n",
    "            interval='5minute'\n",
    "            todaydt=datetime.date.today()\n",
    "            hud_ago=todaydt-timedelta(days=99)\n",
    "            to_date=datetime.date.isoformat(todaydt)\n",
    "            from_date=datetime.date.isoformat(hud_ago)\n",
    "            print('Getting Stock data : '+self.name)\n",
    "            for i2 in tqdm(range(22)):\n",
    "                new_lst = kite.historical_data(self.insttkn, from_date, to_date, interval,continuous=False)\n",
    "                old_lst = new_lst + old_lst\n",
    "                todaydt=todaydt-timedelta(days=100)\n",
    "                hud_ago=hud_ago-timedelta(days=100)\n",
    "                to_date=datetime.date.isoformat(todaydt)\n",
    "                from_date=datetime.date.isoformat(hud_ago)\n",
    "            locals()[self.name+'_DF']=pd.DataFrame(old_lst)\n",
    "            with open(homediraddr+'/pfiles/'+self.name+'_'+'DF'+'.p', 'wb') as fp:\n",
    "                pickle.dump(self.name+'_DF', fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print('Saved file '+homediraddr+'/pfiles/'+self.name+'_'+'DF'+'.p')\n",
    "            print('Loaded : '+self.name+'_'+'DF'+'.p')\n",
    "\n",
    "        with open('scale_dict.p', 'rb') as fp:\n",
    "            scale_dict = pickle.load(fp)\n",
    "            print('Loaded scale_dict')\n",
    "\n",
    "        if(self.name not in scale_dict):\n",
    "            data = locals()[self.name+'_DF'].filter(['close'])  #old_line\n",
    "            dataset = data.values\n",
    "            training_data_len = math.ceil( len(dataset) * .97 )\n",
    "            scaler = MinMaxScaler(feature_range=(0,1))\n",
    "            scaler=scaler.fit(dataset)\n",
    "            scale_dict[self.name]=scaler\n",
    "            with open('scale_dict.p', 'wb') as fp:\n",
    "                pickle.dump(scale_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print(self.name+'_DF'+' added to scale_dict.')\n",
    "            self.scaler=scale_dict[self.name]\n",
    "            print('Loaded scale_dict')\n",
    "        else:\n",
    "            self.scaler=scale_dict[self.name]\n",
    "            print('Loaded scale_dict')\n",
    "            \n",
    "        filecheckpath =homediraddr+'/stkwise_models/'+self.name+'.h5'\n",
    "        if(os.path.isfile(filecheckpath)):\n",
    "            self.model=load_model(filecheckpath)\n",
    "            print('model loaded : '+filecheckpath)\n",
    "        else:\n",
    "            self.model=load_model(homediraddr+'/figs/models/44.h5')\n",
    "            print('model loaded : '+homediraddr+'/figs/models/44.h5')\n",
    "            \n",
    "    def xtrain_def():\n",
    "        dataset = self.df.filter(['close']).values\n",
    "        training_data_len = math.ceil( len(dataset) * .97 )\n",
    "        scaled_data = self.scaler.transform(dataset)\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        train_data = scaled_data[0:training_data_len , :]\n",
    "        for i in range(60, len(train_data)):\n",
    "          x_train.append(train_data[i-60:i, :])\n",
    "          y_train.append(train_data[i, 0])\n",
    "        x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "        test_data = scaled_data[training_data_len - 60: , :]\n",
    "        x_test = []\n",
    "        y_test = dataset[training_data_len:, :]\n",
    "        for i in range(60, len(test_data)):\n",
    "          x_test.append(test_data[i-60:i, :])\n",
    "        x_test = np.array(x_test)\n",
    "        x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "        self.x_train=x_train\n",
    "        self.y_train=y_train\n",
    "        self.x_test=x_test\n",
    "        self.y_test=y_test\n",
    "        print(self.name +' : x_test shape : ',x_test.shape)\n",
    "        print(self.name +' : x_train shape : ',x_train.shape)\n",
    "        \n",
    "    \n",
    "    def plot_predict():\n",
    "        onemodelnameip=self.name\n",
    "        new_df = self.df.filter(['Close'])\n",
    "        scaler=self.scaler\n",
    "        last_60_days = new_df[-180:-120].values\n",
    "        predicted_plot=last_60_days\n",
    "        actual_plot=new_df[-180:].values\n",
    "        try:\n",
    "            last_60_days_scaled = scaler.transform(last_60_days)\n",
    "        except ValueError:\n",
    "            _=0\n",
    "        for jk in range(120):\n",
    "            X_test = []\n",
    "            X_test.append(last_60_days_scaled)\n",
    "            X_test = np.array(X_test)\n",
    "            X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "            pred_price = self.model.predict(X_test)\n",
    "            pred_price_descaled = scaler.inverse_transform(pred_price)\n",
    "            predicted_plot=np.append(predicted_plot,pred_price_descaled)\n",
    "            array_abcd=np.array(pred_price)\n",
    "            asdasdasd=np.append(X_test,array_abcd)\n",
    "            asdasdasd=np.delete(asdasdasd, 0,None)\n",
    "            last_60_days_scaled = np.reshape(asdasdasd, (X_test.shape[0], X_test.shape[1], 1 ))[0]\n",
    "        plot_df=pd.DataFrame(predicted_plot.T)\n",
    "        plot_df2=pd.DataFrame(actual_plot)\n",
    "        result = pd.concat([plot_df, plot_df2], axis=1, sort=False)\n",
    "        result.plot(figsize=(11,5));\n",
    "        self.plotdir=homediraddr+ '/predict_plots/'+self.name\n",
    "        if(not os.path.isdir(self.plotdir)):\n",
    "            os.mkdir(self.plotdir)\n",
    "            print('Directory created : '+self.plotdir)\n",
    "        \n",
    "        filenum=0\n",
    "        filecheckpath = self.plotdir + '/'+self.name+'_'+str(filenum)+'.png'\n",
    "        while os.path.isfile(filecheckpath):\n",
    "            filenum+=1\n",
    "            filecheckpath =self.plotdir + '/'+self.name+'_'+str(filenum)+'.png'\n",
    "        plt.savefig(self.plotdir + '/'+self.name+'_'+str(filenum)+'.png', dpi=100, bbox_inches='tight')\n",
    "\n",
    "    def savemodelh5():\n",
    "        modeldir=homediraddr+ '/stkwise_models/'+self.name\n",
    "        if(not os.path.isdir(modeldir)):\n",
    "            os.mkdir(modeldir)\n",
    "            print('Directory created : '+modeldir)\n",
    "        filenum=0\n",
    "        filecheckpath =modeldir+'/'+self.name+'_'+str(filenum)+'.h5'\n",
    "        while os.path.isfile(filecheckpath):\n",
    "            filenum+=1\n",
    "            filecheckpath =modeldir+'/'+self.name+'_'+str(filenum)+'.h5'\n",
    "        model.save(modeldir+'/'+self.name+'_'+str(filenum)+'.h5')\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fit():\n",
    "        self.load()\n",
    "        self.xtrain_def()\n",
    "        self.plot_predict()\n",
    "        history=self.model.fit(self.x_train,\n",
    "                          self.y_train,\n",
    "                          validation_split=0.03,\n",
    "                          verbose=1,\n",
    "                          batch_size=1, \n",
    "                          epochs=1,\n",
    "                          callbacks=[\n",
    "                                     livelossplot.PlotLossesKeras(),\n",
    "                                     MyCustomCallback(self)\n",
    "                                    ])\n",
    "        self.plot_predict()\n",
    "       \n",
    "  \n",
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, ipobj):\n",
    "        self.ipobj = ipobj\n",
    "    \n",
    "    counter1=0\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        global counter1\n",
    "        counter1+=1\n",
    "        if(counter1%500==0):\n",
    "            self.ipobj.plot_predict()\n",
    "        \n",
    "        if(counter1%2500==0):\n",
    "            self.ipobj.savemodelh5()\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        clear_output(wait=True)      \n",
    "counter1=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdfsdf 12224546 789 1\n",
      "<class '__main__.One_trian_stk_class'>\n"
     ]
    }
   ],
   "source": [
    "x = One_trian_stk_class('sdfsdf', 12224546,1,789)\n",
    "print(x.name, x.insttkn,x.nepochs,x.nbatch)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stkdef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
